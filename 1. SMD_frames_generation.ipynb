{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and play with the Singapore dataset\n",
    "\n",
    "This is a jupyter notebook to experiment and generate the frames of the videos in the Singapore Maritime \n",
    "Dataset (SMD) as jpg images. Also, histograms of the objects heights, widths and areas as a ration of the total images are generated.\n",
    "\n",
    "Dataset available here: https://sites.google.com/site/dilipprasad/home/singapore-maritime-dataset\n",
    "\n",
    "If this dataset is used please cite it as:\n",
    "\n",
    "D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, and C. Quek, \"Video Processing from Electro-optical Sensors for Object Detection and Tracking in Maritime Environment: A Survey,\" IEEE Transactions on Intelligent Transportation Systems (IEEE), 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the videos paths for both onboard and onshore data and generate dictionaries with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths for the video files and ground truth files\n",
    "# by default it is assumed that the paths are in the same folders\n",
    "# as the notebook\n",
    "\n",
    "VIDEOS_PATH_ONSHORE = \"./SMD_Plus/VIS_Onshore/Videos\"\n",
    "OBJECT_ANNOTATIONS_ONSHORE_PATH = \"./SMD_Plus/VIS_Onshore/ObjectGT\"\n",
    "VIDEO_FRAMES_PATH_ONSHORE = './SMD_Plus/VIS_Onshore_frames/'\n",
    "\n",
    "VIDEOS_PATH_ONBOARD = \"./SMD_Plus/VIS_Onboard/Videos\"\n",
    "OBJECT_ANNOTATIONS_ONBOARD_PATH = \"./SMD_Plus/VIS_Onboard/ObjectGT\"\n",
    "VIDEO_FRAMES_PATH_ONBOARD = './SMD_Plus/VIS_Onboard_frames/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files_onshore = [join(VIDEOS_PATH_ONSHORE, f) for f in listdir(VIDEOS_PATH_ONSHORE) \n",
    "                       if isfile(join(VIDEOS_PATH_ONSHORE, f))]\n",
    "\n",
    "video_files_onboard = [join(VIDEOS_PATH_ONBOARD, f) for f in listdir(VIDEOS_PATH_ONBOARD) \n",
    "                       if isfile(join(VIDEOS_PATH_ONBOARD, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries for each video in the form video_name:video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_files_onshore_dict = {}\n",
    "for f in listdir(VIDEOS_PATH_ONSHORE):\n",
    "    if isfile(join(VIDEOS_PATH_ONSHORE, f)):\n",
    "        video_files_onshore_dict[f.split('.')[0]] = join(VIDEOS_PATH_ONSHORE, f)\n",
    "        \n",
    "video_files_onboard_dict = {}\n",
    "for f in listdir(VIDEOS_PATH_ONBOARD):\n",
    "    if isfile(join(VIDEOS_PATH_ONBOARD, f)):\n",
    "        video_files_onboard_dict[f.split('.')[0]] = join(VIDEOS_PATH_ONBOARD, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ground truth files paths for both onboard and onshore data and generate dictionaries with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_gt_files_onshore_dict = {}\n",
    "for f in listdir(OBJECT_ANNOTATIONS_ONSHORE_PATH):\n",
    "    if isfile(join(OBJECT_ANNOTATIONS_ONSHORE_PATH, f)):\n",
    "        object_gt_files_onshore_dict[f.split('.')[0].replace('_ObjectGT','')] = join(OBJECT_ANNOTATIONS_ONSHORE_PATH, f)\n",
    "        \n",
    "object_gt_files_onboard_dict = {}\n",
    "for f in listdir(OBJECT_ANNOTATIONS_ONBOARD_PATH):\n",
    "    if isfile(join(OBJECT_ANNOTATIONS_ONBOARD_PATH, f)):\n",
    "        object_gt_files_onboard_dict[f.split('.')[0].replace('_ObjectGT','')] = join(OBJECT_ANNOTATIONS_ONBOARD_PATH, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some sanity checks to see if there are equal numbers of videos and ground truth files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of onshore videos:  40\n",
      "Number of onshore ground truth files:  40\n",
      "Number of onboard videos:  11\n",
      "Number of onboard ground truth files:  11\n"
     ]
    }
   ],
   "source": [
    "print('Number of onshore videos: ', len(video_files_onshore_dict))\n",
    "print('Number of onshore ground truth files: ', len(object_gt_files_onshore_dict))\n",
    "\n",
    "print('Number of onboard videos: ', len(video_files_onboard_dict))\n",
    "print('Number of onboard ground truth files: ', len(object_gt_files_onboard_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are videos without ground truth files. These unlabelled data might be good for testing later. Let's find these videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabelled onshore videos:  []\n",
      "Unlabelled onbord videos:  []\n",
      "\n",
      "Size of video dictionaries after removing the videos without ground truth:\n",
      "Number of onshore videos:  40\n",
      "Number of onshore ground truth files:  40\n",
      "Number of onboard videos:  11\n",
      "Number of onboard ground truth files:  11\n"
     ]
    }
   ],
   "source": [
    "# ground truth files are missing - find the corresponding videos\n",
    "missing_files_onshore = []\n",
    "for key in video_files_onshore_dict.keys():\n",
    "    if key not in object_gt_files_onshore_dict:\n",
    "        missing_files_onshore.append(key)\n",
    "        \n",
    "print(\"Unlabelled onshore videos: \", missing_files_onshore)\n",
    "\n",
    "missing_files_onboard = []\n",
    "for key in video_files_onboard_dict.keys():\n",
    "    if key not in object_gt_files_onboard_dict:\n",
    "        missing_files_onboard.append(key)\n",
    "        \n",
    "print(\"Unlabelled onbord videos: \", missing_files_onboard)\n",
    "\n",
    "# set whether to remove or not the missing videos from the frames generation later\n",
    "remove_missing_files = True\n",
    "if remove_missing_files:\n",
    "    for key in missing_files_onshore:\n",
    "        del video_files_onshore_dict[key]\n",
    "    for key in missing_files_onboard:\n",
    "        del video_files_onboard_dict[key]\n",
    "        \n",
    "    print()\n",
    "    print('Size of video dictionaries after removing the videos without ground truth:')\n",
    "    print('Number of onshore videos: ', len(video_files_onshore_dict))\n",
    "    print('Number of onshore ground truth files: ', len(object_gt_files_onshore_dict))\n",
    "\n",
    "    print('Number of onboard videos: ', len(video_files_onboard_dict))\n",
    "    print('Number of onboard ground truth files: ', len(object_gt_files_onboard_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_files_onshore_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ALL frames of the videos into jpg images\n",
    " This is code to convert each video frame into a jpg image.\n",
    " \n",
    " The first cell is for converting only one video. The second is for converting all the videos in a folder into jpg images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert ALL on shore videos into images with 1 image per frame\n",
    "for video_key in video_files_onshore_dict:\n",
    "    vidcap = cv2.VideoCapture(video_files_onshore_dict.get(video_key))\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "      cv2.imwrite(VIDEO_FRAMES_PATH_ONSHORE + video_key + \"_frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "      success,image = vidcap.read()\n",
    "      #print('Read a new frame: ', success)\n",
    "      count += 1\n",
    "    print(\"Derived %d frames\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ALL on board videos into images with 1 image per frame\n",
    "for video_key in video_files_onboard_dict:\n",
    "    vidcap = cv2.VideoCapture(video_files_onboard_dict.get(video_key))\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "      cv2.imwrite(VIDEO_FRAMES_PATH_ONBOARD + video_key + \"_frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "      success,image = vidcap.read()\n",
    "      #print('Read a new frame: ', success)\n",
    "      count += 1\n",
    "    print(\"Derived %d frames\" % count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert every N frame of a video into jpg image and split into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean to determine whether to have all frames in one or separate folders (onshore/onboard/nir)\n",
    "SEPARATE_FOLDERS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './SMD_Plus/train'\n",
    "TEST_PATH = './SMD_Plus/test'\n",
    "VIDEO_FRAMES_FOLDER_NAME_ONSHORE = 'VIS_Onshore'\n",
    "VIDEO_FRAMES_FOLDER_NAME_ONBOARD = 'VIS_Onboard'\n",
    "\n",
    "if SEPARATE_FOLDERS:\n",
    "    TRAIN_ONSHORE = join(TRAIN_PATH, VIDEO_FRAMES_FOLDER_NAME_ONSHORE)\n",
    "    TEST_ONSHORE = join(TEST_PATH, VIDEO_FRAMES_FOLDER_NAME_ONSHORE)\n",
    "    TRAIN_ONBOARD = join(TRAIN_PATH, VIDEO_FRAMES_FOLDER_NAME_ONBOARD)\n",
    "    TEST_ONBOARD = join(TEST_PATH, VIDEO_FRAMES_FOLDER_NAME_ONBOARD)\n",
    "\n",
    "    folder_names = [TRAIN_PATH, TEST_PATH, TRAIN_ONSHORE, TEST_ONSHORE, TRAIN_ONBOARD, TEST_ONBOARD]\n",
    "else:\n",
    "    folder_names = [TRAIN_PATH, TEST_PATH]\n",
    "\n",
    "# first create the folders if they don't exist\n",
    "for folder_name in folder_names:\n",
    "    if not (os.path.isdir(folder_name)):\n",
    "        os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_videos_to_frames(video_dict, paths, frame_space=20, train_test_split=0.8):\n",
    "    \"\"\"\n",
    "    Helper function to convert any video frames into jpg images and split them into training and test dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    video_dict : dictionary in the form <video_name>:<video_path>\n",
    "    \n",
    "    paths : tuple of the training and test paths to save the images. If both a set to the same path\n",
    "            then all the generated frames will be place in this (same) directory.\n",
    "            \n",
    "    frame_space : the space between the generated frames. Default is 5.\n",
    "    \n",
    "    train_test_split : the ration to split the frames into train and test datasets. Default is 0.9\n",
    "    \"\"\"\n",
    "    train_path = paths[0]\n",
    "    test_path = paths[1]\n",
    "    for video_key in video_dict:\n",
    "        vidcap = cv2.VideoCapture(video_dict.get(video_key))\n",
    "        \n",
    "        # get total frames of video\n",
    "        total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        success,image = vidcap.read()\n",
    "        count = 0\n",
    "        frame_count = 0\n",
    "        success = True\n",
    "        while success:\n",
    "            if count % frame_space == 0:\n",
    "                if (count <= train_test_split*total_frames):\n",
    "                    cv2.imwrite(join(train_path, video_key) + \"_frame%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "                else:\n",
    "                    cv2.imwrite(join(test_path, video_key) + \"_frame%d.jpg\" % count, image)\n",
    "                frame_count += 1\n",
    "            success,image = vidcap.read()\n",
    "            #print('Read a new frame: ', success)\n",
    "            count += 1\n",
    "        print(\"Derived %d frames\" % frame_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived 15 frames\n",
      "Derived 30 frames\n",
      "Derived 14 frames\n",
      "Derived 15 frames\n",
      "Derived 15 frames\n",
      "Derived 15 frames\n",
      "Derived 30 frames\n",
      "Derived 30 frames\n",
      "Derived 13 frames\n",
      "Derived 15 frames\n",
      "Derived 30 frames\n",
      "Derived 26 frames\n",
      "Derived 32 frames\n",
      "Derived 35 frames\n",
      "Derived 26 frames\n",
      "Derived 27 frames\n",
      "Derived 26 frames\n",
      "Derived 13 frames\n",
      "Derived 28 frames\n",
      "Derived 15 frames\n",
      "Derived 22 frames\n",
      "Derived 26 frames\n",
      "Derived 30 frames\n",
      "Derived 30 frames\n",
      "Derived 25 frames\n",
      "Derived 15 frames\n",
      "Derived 30 frames\n",
      "Derived 27 frames\n",
      "Derived 17 frames\n",
      "Derived 13 frames\n",
      "Derived 30 frames\n",
      "Derived 24 frames\n",
      "Derived 32 frames\n",
      "Derived 14 frames\n",
      "Derived 16 frames\n",
      "Derived 31 frames\n",
      "Derived 27 frames\n",
      "Derived 28 frames\n",
      "Derived 23 frames\n",
      "Derived 23 frames\n",
      "Derived 14 frames\n",
      "Derived 6 frames\n",
      "Derived 30 frames\n",
      "Derived 25 frames\n",
      "Derived 50 frames\n",
      "Derived 29 frames\n",
      "Derived 21 frames\n",
      "Derived 16 frames\n",
      "Derived 27 frames\n",
      "Derived 11 frames\n",
      "Derived 24 frames\n"
     ]
    }
   ],
   "source": [
    "if SEPARATE_FOLDERS:\n",
    "    convert_videos_to_frames(video_files_onboard_dict, [TRAIN_ONBOARD, TEST_ONBOARD])\n",
    "    convert_videos_to_frames(video_files_onshore_dict, [TRAIN_ONSHORE, TEST_ONSHORE])\n",
    "else:\n",
    "    convert_videos_to_frames(video_files_onboard_dict, [TRAIN_PATH, TEST_PATH])\n",
    "    convert_videos_to_frames(video_files_onshore_dict, [TRAIN_PATH, TEST_PATH])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
